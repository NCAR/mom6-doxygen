<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>DART CAM-SE: /Users/hkershaw/DART/Doxygen/DART/assimilation_code/modules/utilities/mpif08_utilities_mod.f90 File Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">DART CAM-SE
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="dir_4b8c547e68dcddf016c38a1b08f5c7a9.html">DART</a></li><li class="navelem"><a class="el" href="dir_45bc59af759ed40ec308edf679618634.html">assimilation_code</a></li><li class="navelem"><a class="el" href="dir_b72b743c828fcd5d2c7dc1877007622f.html">modules</a></li><li class="navelem"><a class="el" href="dir_1c85c344e6f6b1c23732befc9a4d92f1.html">utilities</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#nested-classes">Data Types</a> &#124;
<a href="#namespaces">Modules</a> &#124;
<a href="#func-members">Functions/Subroutines</a>  </div>
  <div class="headertitle"><div class="title">mpif08_utilities_mod.f90 File Reference</div></div>
</div><!--header-->
<div class="contents">

<p><a href="mpif08__utilities__mod_8f90_source.html">Go to the source code of this file.</a></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="nested-classes" name="nested-classes"></a>
Data Types</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">interface &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="interfacempi__utilities__mod_1_1sum__across__tasks.html">mpi_utilities_mod::sum_across_tasks</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="namespaces" name="namespaces"></a>
Modules</h2></td></tr>
<tr class="memitem:namespacempi__utilities__mod" id="r_namespacempi__utilities__mod"><td class="memItemLeft" align="right" valign="top">module &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html">mpi_utilities_mod</a></td></tr>
<tr class="memdesc:namespacempi__utilities__mod"><td class="mdescLeft">&#160;</td><td class="mdescRight">A collection of interfaces to the MPI (Message Passing Interface) multi-processor communication library routines. <br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions/Subroutines</h2></td></tr>
<tr class="memitem:a4d8d536d47f9c365f24da2d2f477ceb6" id="r_a4d8d536d47f9c365f24da2d2f477ceb6"><td class="memItemLeft" align="right" valign="top">subroutine, public&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#a4d8d536d47f9c365f24da2d2f477ceb6">mpi_utilities_mod::initialize_mpi_utilities</a> (progname, alternatename, communicator)</td></tr>
<tr class="memdesc:a4d8d536d47f9c365f24da2d2f477ceb6"><td class="mdescLeft">&#160;</td><td class="mdescRight">Initialize MPI and query it for global information. Make a duplicate communicator so that any user code which wants to call MPI will not interfere with any outstanding asynchronous requests, accidental tag matches, etc. This routine must be called before any other routine in this file, and it should not be called more than once (but it does have defensive code in case that happens.)  <br /></td></tr>
<tr class="separator:a4d8d536d47f9c365f24da2d2f477ceb6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a251e06d2d116c03e25552ca747088795" id="r_a251e06d2d116c03e25552ca747088795"><td class="memItemLeft" align="right" valign="top">subroutine, public&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#a251e06d2d116c03e25552ca747088795">mpi_utilities_mod::finalize_mpi_utilities</a> (callfinalize, async)</td></tr>
<tr class="memdesc:a251e06d2d116c03e25552ca747088795"><td class="mdescLeft">&#160;</td><td class="mdescRight">Shut down MPI cleanly. This must be done before the program exits; on some implementations of MPI the final I/O flushes are not done until this is called. The optional argument can prevent us from calling MPI_Finalize, so that user code can continue to use MPI after this returns. Calling other routines in this file after calling finalize will invalidate your warranty.  <br /></td></tr>
<tr class="separator:a251e06d2d116c03e25552ca747088795"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae8ccd09884b18c6d167291247089bd27" id="r_ae8ccd09884b18c6d167291247089bd27"><td class="memItemLeft" align="right" valign="top">integer function, public&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#ae8ccd09884b18c6d167291247089bd27">mpi_utilities_mod::task_count</a> ()</td></tr>
<tr class="memdesc:ae8ccd09884b18c6d167291247089bd27"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return the total number of MPI tasks. e.g. if the number of tasks is 4, it returns 4. (The actual task numbers are 0-3.)  <br /></td></tr>
<tr class="separator:ae8ccd09884b18c6d167291247089bd27"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae08b6543f55b372bd6addea0eb1e04a0" id="r_ae08b6543f55b372bd6addea0eb1e04a0"><td class="memItemLeft" align="right" valign="top">integer function, public&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#ae08b6543f55b372bd6addea0eb1e04a0">mpi_utilities_mod::my_task_id</a> ()</td></tr>
<tr class="memdesc:ae08b6543f55b372bd6addea0eb1e04a0"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return my unique task id. Values run from 0 to N-1 (where N is the total number of MPI tasks.  <br /></td></tr>
<tr class="separator:ae08b6543f55b372bd6addea0eb1e04a0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a19228477ddb48773bf4fb3858c997d82" id="r_a19228477ddb48773bf4fb3858c997d82"><td class="memItemLeft" align="right" valign="top">subroutine, public&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#a19228477ddb48773bf4fb3858c997d82">mpi_utilities_mod::task_sync</a> ()</td></tr>
<tr class="memdesc:a19228477ddb48773bf4fb3858c997d82"><td class="mdescLeft">&#160;</td><td class="mdescRight">Synchronize all tasks. This subroutine does not return until all tasks execute this line of code.  <br /></td></tr>
<tr class="separator:a19228477ddb48773bf4fb3858c997d82"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af398cd54ac6d3c0b7f7f10be44eaf953" id="r_af398cd54ac6d3c0b7f7f10be44eaf953"><td class="memItemLeft" align="right" valign="top">subroutine, public&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#af398cd54ac6d3c0b7f7f10be44eaf953">mpi_utilities_mod::send_to</a> (dest_id, srcarray, time, label)</td></tr>
<tr class="memdesc:af398cd54ac6d3c0b7f7f10be44eaf953"><td class="mdescLeft">&#160;</td><td class="mdescRight">Send the srcarray to the destination id. If time is specified, it is also sent in a separate communications call. <br  />
 This is a synchronous call; it will not return until the destination has called receive to accept the data. If the send_to/receive_from calls are not paired correctly the code will hang.  <br /></td></tr>
<tr class="separator:af398cd54ac6d3c0b7f7f10be44eaf953"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adec13c965903d1181b4379234d4cc602" id="r_adec13c965903d1181b4379234d4cc602"><td class="memItemLeft" align="right" valign="top">subroutine, public&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#adec13c965903d1181b4379234d4cc602">mpi_utilities_mod::receive_from</a> (src_id, destarray, time, label)</td></tr>
<tr class="memdesc:adec13c965903d1181b4379234d4cc602"><td class="mdescLeft">&#160;</td><td class="mdescRight">Receive data into the destination array from the src task. If time is specified, it is received in a separate communications call. <br  />
 This is a synchronous call; it will not return until the source has sent the data. If the send_to/receive_from calls are not paired correctly the code will hang.  <br /></td></tr>
<tr class="separator:adec13c965903d1181b4379234d4cc602"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a49897132fa87ca79441f794ad3cc6642" id="r_a49897132fa87ca79441f794ad3cc6642"><td class="memItemLeft" align="right" valign="top">subroutine, public&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#a49897132fa87ca79441f794ad3cc6642">mpi_utilities_mod::array_broadcast</a> (array, root, icount)</td></tr>
<tr class="memdesc:a49897132fa87ca79441f794ad3cc6642"><td class="mdescLeft">&#160;</td><td class="mdescRight">The data array values on the root task will be broadcast to every other task. When this routine returns, all tasks will have the contents of the root array in their own arrays. Thus 'array' is intent(in) on root, and intent(out) on all other tasks.  <br /></td></tr>
<tr class="separator:a49897132fa87ca79441f794ad3cc6642"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adde20af06e780ec70378db008a88b951" id="r_adde20af06e780ec70378db008a88b951"><td class="memItemLeft" align="right" valign="top">logical function, public&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#adde20af06e780ec70378db008a88b951">mpi_utilities_mod::iam_task0</a> ()</td></tr>
<tr class="memdesc:adde20af06e780ec70378db008a88b951"><td class="mdescLeft">&#160;</td><td class="mdescRight">Return .TRUE. if my local task id is 0, .FALSE. otherwise. (Task numbers in MPI start at 0, contrary to the rules of polite fortran.)  <br /></td></tr>
<tr class="separator:adde20af06e780ec70378db008a88b951"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5d695a71f9877df21d7ff2a39dba9821" id="r_a5d695a71f9877df21d7ff2a39dba9821"><td class="memItemLeft" align="right" valign="top">subroutine, public&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#a5d695a71f9877df21d7ff2a39dba9821">mpi_utilities_mod::broadcast_send</a> (from, array1, array2, array3, array4, array5, scalar1, scalar2, scalar3, scalar4, scalar5)</td></tr>
<tr class="memdesc:a5d695a71f9877df21d7ff2a39dba9821"><td class="mdescLeft">&#160;</td><td class="mdescRight">this must be paired with the same number of broadcast_recv()s on all other tasks. it will not return until all tasks in the communications group have made the call.  <br /></td></tr>
<tr class="separator:a5d695a71f9877df21d7ff2a39dba9821"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abf5f213db52aac32b7f0bdf774d1a1d9" id="r_abf5f213db52aac32b7f0bdf774d1a1d9"><td class="memItemLeft" align="right" valign="top">subroutine, public&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#abf5f213db52aac32b7f0bdf774d1a1d9">mpi_utilities_mod::broadcast_recv</a> (from, array1, array2, array3, array4, array5, scalar1, scalar2, scalar3, scalar4, scalar5)</td></tr>
<tr class="memdesc:abf5f213db52aac32b7f0bdf774d1a1d9"><td class="mdescLeft">&#160;</td><td class="mdescRight">this must be paired with a single broadcast_send() on one other task, and broadcast_recv() on all other tasks, and it must match exactly the number of args in the sending call. it will not return until all tasks in the communications group have made the call.  <br /></td></tr>
<tr class="separator:abf5f213db52aac32b7f0bdf774d1a1d9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa589a4428dd48aa7ee2f5fe38c57d420" id="r_aa589a4428dd48aa7ee2f5fe38c57d420"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#aa589a4428dd48aa7ee2f5fe38c57d420">mpi_utilities_mod::countup</a> (array1, array2, array3, array4, array5, scalar1, scalar2, scalar3, scalar4, scalar5, numitems, morethanone, doscalar)</td></tr>
<tr class="memdesc:aa589a4428dd48aa7ee2f5fe38c57d420"><td class="mdescLeft">&#160;</td><td class="mdescRight">figure out how many items are in the specified arrays, total. also note if there's more than a single array (array1) to send, and if there are any scalars specified.  <br /></td></tr>
<tr class="separator:aa589a4428dd48aa7ee2f5fe38c57d420"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a834a62e1028f5fd9615c8c8eacdf8b16" id="r_a834a62e1028f5fd9615c8c8eacdf8b16"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#a834a62e1028f5fd9615c8c8eacdf8b16">mpi_utilities_mod::packit</a> (buf, array1, array2, array3, array4, array5, doscalar, scalar1, scalar2, scalar3, scalar4, scalar5)</td></tr>
<tr class="memdesc:a834a62e1028f5fd9615c8c8eacdf8b16"><td class="mdescLeft">&#160;</td><td class="mdescRight">pack multiple small arrays into a single buffer before sending.  <br /></td></tr>
<tr class="separator:a834a62e1028f5fd9615c8c8eacdf8b16"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0e57408a9b33dead7951adaee0c0c010" id="r_a0e57408a9b33dead7951adaee0c0c010"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#a0e57408a9b33dead7951adaee0c0c010">mpi_utilities_mod::unpackit</a> (buf, array1, array2, array3, array4, array5, doscalar, scalar1, scalar2, scalar3, scalar4, scalar5)</td></tr>
<tr class="memdesc:a0e57408a9b33dead7951adaee0c0c010"><td class="mdescLeft">&#160;</td><td class="mdescRight">unpack multiple small arrays from a single buffer after receiving.  <br /></td></tr>
<tr class="separator:a0e57408a9b33dead7951adaee0c0c010"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af7743a527abd2f96bf0eafbd3867dbd2" id="r_af7743a527abd2f96bf0eafbd3867dbd2"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#af7743a527abd2f96bf0eafbd3867dbd2">mpi_utilities_mod::packscalar</a> (local, scalar1, scalar2, scalar3, scalar4, scalar5)</td></tr>
<tr class="memdesc:af7743a527abd2f96bf0eafbd3867dbd2"><td class="mdescLeft">&#160;</td><td class="mdescRight">for any values specified, pack into a single array  <br /></td></tr>
<tr class="separator:af7743a527abd2f96bf0eafbd3867dbd2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2ffcaac3a89f37f3c1b9ac4123313d29" id="r_a2ffcaac3a89f37f3c1b9ac4123313d29"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#a2ffcaac3a89f37f3c1b9ac4123313d29">mpi_utilities_mod::unpackscalar</a> (local, scalar1, scalar2, scalar3, scalar4, scalar5)</td></tr>
<tr class="memdesc:a2ffcaac3a89f37f3c1b9ac4123313d29"><td class="mdescLeft">&#160;</td><td class="mdescRight">for any values specified, unpack from a single array  <br /></td></tr>
<tr class="separator:a2ffcaac3a89f37f3c1b9ac4123313d29"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a801258b9543adc92a73637ebc86904f9" id="r_a801258b9543adc92a73637ebc86904f9"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#a801258b9543adc92a73637ebc86904f9">mpi_utilities_mod::sum_across_tasks_int4</a> (addend, sum)</td></tr>
<tr class="memdesc:a801258b9543adc92a73637ebc86904f9"><td class="mdescLeft">&#160;</td><td class="mdescRight">take values from each task, add them, and return the sum to all tasks. integer version  <br /></td></tr>
<tr class="separator:a801258b9543adc92a73637ebc86904f9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3ee66ce252ef917967a2c1d12db26620" id="r_a3ee66ce252ef917967a2c1d12db26620"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#a3ee66ce252ef917967a2c1d12db26620">mpi_utilities_mod::sum_across_tasks_int8</a> (addend, sum)</td></tr>
<tr class="memdesc:a3ee66ce252ef917967a2c1d12db26620"><td class="mdescLeft">&#160;</td><td class="mdescRight">take values from each task, add them, and return the sum to all tasks. long integer version.  <br /></td></tr>
<tr class="separator:a3ee66ce252ef917967a2c1d12db26620"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2e54a551c64c4a01d80e1e8471deea67" id="r_a2e54a551c64c4a01d80e1e8471deea67"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#a2e54a551c64c4a01d80e1e8471deea67">mpi_utilities_mod::sum_across_tasks_real</a> (addend, sum)</td></tr>
<tr class="memdesc:a2e54a551c64c4a01d80e1e8471deea67"><td class="mdescLeft">&#160;</td><td class="mdescRight">take values from each task, add them, and return the sum to all tasks. real version.  <br /></td></tr>
<tr class="separator:a2e54a551c64c4a01d80e1e8471deea67"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5dd51bd03e7b1ac3711b8ffb434e8db9" id="r_a5dd51bd03e7b1ac3711b8ffb434e8db9"><td class="memItemLeft" align="right" valign="top">subroutine, public&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#a5dd51bd03e7b1ac3711b8ffb434e8db9">mpi_utilities_mod::send_sum_to</a> (local_val, task, global_val)</td></tr>
<tr class="memdesc:a5dd51bd03e7b1ac3711b8ffb434e8db9"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sum array items across all tasks and send results in an array of same size to one task.  <br /></td></tr>
<tr class="separator:a5dd51bd03e7b1ac3711b8ffb434e8db9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aacbda19146b5ad3f0a75eb6cbfd57393" id="r_aacbda19146b5ad3f0a75eb6cbfd57393"><td class="memItemLeft" align="right" valign="top">subroutine, public&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#aacbda19146b5ad3f0a75eb6cbfd57393">mpi_utilities_mod::send_minmax_to</a> (minmax, task, global_val)</td></tr>
<tr class="memdesc:aacbda19146b5ad3f0a75eb6cbfd57393"><td class="mdescLeft">&#160;</td><td class="mdescRight">Collect global min and max values on one task.  <br /></td></tr>
<tr class="separator:aacbda19146b5ad3f0a75eb6cbfd57393"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6186c753fcfb54ac3ca322c74573992e" id="r_a6186c753fcfb54ac3ca322c74573992e"><td class="memItemLeft" align="right" valign="top">subroutine, public&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#a6186c753fcfb54ac3ca322c74573992e">mpi_utilities_mod::all_reduce_min_max</a> (min_var, max_var, num_elements)</td></tr>
<tr class="memdesc:a6186c753fcfb54ac3ca322c74573992e"><td class="mdescLeft">&#160;</td><td class="mdescRight">cover routine which is deprecated. when all user code replaces this with <a class="el" href="namespacempi__utilities__mod.html#a4f8d101d8c90b83ed4392eac20339b37" title="Find min and max of each element of an array, put the result on every task. Overwrites arrays min_var...">broadcast_minmax()</a>, remove this.  <br /></td></tr>
<tr class="separator:a6186c753fcfb54ac3ca322c74573992e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4f8d101d8c90b83ed4392eac20339b37" id="r_a4f8d101d8c90b83ed4392eac20339b37"><td class="memItemLeft" align="right" valign="top">subroutine, public&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#a4f8d101d8c90b83ed4392eac20339b37">mpi_utilities_mod::broadcast_minmax</a> (min_var, max_var, num_elements)</td></tr>
<tr class="memdesc:a4f8d101d8c90b83ed4392eac20339b37"><td class="mdescLeft">&#160;</td><td class="mdescRight">Find min and max of each element of an array, put the result on every task. Overwrites arrays min_var, max_var with the minimum and maximum for each element across all tasks.  <br /></td></tr>
<tr class="separator:a4f8d101d8c90b83ed4392eac20339b37"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a11898801659b52a8b452d89672fb7e8e" id="r_a11898801659b52a8b452d89672fb7e8e"><td class="memItemLeft" align="right" valign="top">subroutine, public&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#a11898801659b52a8b452d89672fb7e8e">mpi_utilities_mod::broadcast_flag</a> (flag, root)</td></tr>
<tr class="memdesc:a11898801659b52a8b452d89672fb7e8e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Broadcast logical.  <br /></td></tr>
<tr class="separator:a11898801659b52a8b452d89672fb7e8e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2ab78b67e9b7eaed49c7616842a979d9" id="r_a2ab78b67e9b7eaed49c7616842a979d9"><td class="memItemLeft" align="right" valign="top">subroutine, public&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#a2ab78b67e9b7eaed49c7616842a979d9">mpi_utilities_mod::block_task</a> ()</td></tr>
<tr class="memdesc:a2ab78b67e9b7eaed49c7616842a979d9"><td class="mdescLeft">&#160;</td><td class="mdescRight">block by reading a named pipe file until some other task writes a string into it. this ensures the task is not spinning and using CPU cycles, but is asleep waiting in the kernel. one subtlety with this approach is that even though named pipes are created in the filesystem, they are implemented in the kernel, so on a multiprocessor machine the write into the pipe file must occur on the same PE as the reader is waiting. see the 'wakeup_filter' program for the MPI job which spreads out on all the PEs for this job and writes into the file from the correct PE.  <br /></td></tr>
<tr class="separator:a2ab78b67e9b7eaed49c7616842a979d9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9e2b3052d82ea46f183fccc3a00d7ac1" id="r_a9e2b3052d82ea46f183fccc3a00d7ac1"><td class="memItemLeft" align="right" valign="top">subroutine, public&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#a9e2b3052d82ea46f183fccc3a00d7ac1">mpi_utilities_mod::restart_task</a> ()</td></tr>
<tr class="memdesc:a9e2b3052d82ea46f183fccc3a00d7ac1"><td class="mdescLeft">&#160;</td><td class="mdescRight">companion to block_task. must be called by a different executable and it writes into the named pipes to restart the waiting task.  <br /></td></tr>
<tr class="separator:a9e2b3052d82ea46f183fccc3a00d7ac1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af3ae1106934e1ef9f30e16a1d0911309" id="r_af3ae1106934e1ef9f30e16a1d0911309"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#af3ae1106934e1ef9f30e16a1d0911309">mpi_utilities_mod::finished_task</a> (async)</td></tr>
<tr class="memdesc:af3ae1106934e1ef9f30e16a1d0911309"><td class="mdescLeft">&#160;</td><td class="mdescRight">must be called when filter is exiting so calling script knows the job is over.  <br /></td></tr>
<tr class="separator:af3ae1106934e1ef9f30e16a1d0911309"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3830a5276fa6c01e95f31ef8ad53fef1" id="r_a3830a5276fa6c01e95f31ef8ad53fef1"><td class="memItemLeft" align="right" valign="top">integer function, public&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#a3830a5276fa6c01e95f31ef8ad53fef1">mpi_utilities_mod::shell_execute</a> (execute_string, serialize)</td></tr>
<tr class="memdesc:a3830a5276fa6c01e95f31ef8ad53fef1"><td class="mdescLeft">&#160;</td><td class="mdescRight">Use the system() command to execute a command string. Will wait for the command to complete and returns an error code unless you end the command with &amp; to put it into background. Function which returns the rc of the command, 0 being all is ok.  <br /></td></tr>
<tr class="separator:a3830a5276fa6c01e95f31ef8ad53fef1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7be71c4be41ac0c517b25519d690ff31" id="r_a7be71c4be41ac0c517b25519d690ff31"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#a7be71c4be41ac0c517b25519d690ff31">mpi_utilities_mod::do_system</a> (execute, rc)</td></tr>
<tr class="memdesc:a7be71c4be41ac0c517b25519d690ff31"><td class="mdescLeft">&#160;</td><td class="mdescRight">wrapper so you only have to make this work in a single place 'shell_name' is a namelist item and normally is the null string. on at least on cray system, the compute nodes only had one type of shell and you had to specify it.  <br /></td></tr>
<tr class="separator:a7be71c4be41ac0c517b25519d690ff31"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1a7a3eff45a3f677d024a78063b836d8" id="r_a1a7a3eff45a3f677d024a78063b836d8"><td class="memItemLeft" align="right" valign="top">subroutine, public&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#a1a7a3eff45a3f677d024a78063b836d8">mpi_utilities_mod::sleep_seconds</a> (naplength)</td></tr>
<tr class="memdesc:a1a7a3eff45a3f677d024a78063b836d8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Wrapper for the sleep command. Argument is a real in seconds. Different systems have different lower resolutions for the minimum time it will sleep. Subroutine, no return value.  <br /></td></tr>
<tr class="separator:a1a7a3eff45a3f677d024a78063b836d8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab4ac2406ce8edd642120789bf7096867" id="r_ab4ac2406ce8edd642120789bf7096867"><td class="memItemLeft" align="right" valign="top">subroutine, public&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#ab4ac2406ce8edd642120789bf7096867">mpi_utilities_mod::start_mpi_timer</a> (base)</td></tr>
<tr class="memdesc:ab4ac2406ce8edd642120789bf7096867"><td class="mdescLeft">&#160;</td><td class="mdescRight">start a time block. call with different argument to start multiple or nested timers. same argument must be supplied to read_timer function to get elapsed time since that timer was set. contrast this with 'start_timer/read_timer' in the utils module which returns elapsed seconds. this returns whatever units the mpi wtime() function returns.  <br /></td></tr>
<tr class="separator:ab4ac2406ce8edd642120789bf7096867"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7c1b206a9f66585ef6ed9b04c033ae25" id="r_a7c1b206a9f66585ef6ed9b04c033ae25"><td class="memItemLeft" align="right" valign="top">real(digits12) function, public&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#a7c1b206a9f66585ef6ed9b04c033ae25">mpi_utilities_mod::read_mpi_timer</a> (base)</td></tr>
<tr class="memdesc:a7c1b206a9f66585ef6ed9b04c033ae25"><td class="mdescLeft">&#160;</td><td class="mdescRight">return the time since the last call to start_timer(). can call multiple times to get running times. call with a different base for nested timers.  <br /></td></tr>
<tr class="separator:a7c1b206a9f66585ef6ed9b04c033ae25"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adc06ffeb863472ff64785cbd82cc36d7" id="r_adc06ffeb863472ff64785cbd82cc36d7"><td class="memItemLeft" align="right" valign="top">integer function, public&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#adc06ffeb863472ff64785cbd82cc36d7">mpi_utilities_mod::get_dart_mpi_comm</a> ()</td></tr>
<tr class="memdesc:adc06ffeb863472ff64785cbd82cc36d7"><td class="mdescLeft">&#160;</td><td class="mdescRight">return our communicator  <br /></td></tr>
<tr class="separator:adc06ffeb863472ff64785cbd82cc36d7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7c749bce2fa49329886346b37c1692ba" id="r_a7c749bce2fa49329886346b37c1692ba"><td class="memItemLeft" align="right" valign="top">subroutine, public&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#a7c749bce2fa49329886346b37c1692ba">mpi_utilities_mod::get_from_mean</a> (owner, window, mindex, x)</td></tr>
<tr class="separator:a7c749bce2fa49329886346b37c1692ba"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0548d3049128f34500d9212b7f946351" id="r_a0548d3049128f34500d9212b7f946351"><td class="memItemLeft" align="right" valign="top">subroutine, public&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#a0548d3049128f34500d9212b7f946351">mpi_utilities_mod::get_from_fwd</a> (owner, window, mindex, num_rows, x)</td></tr>
<tr class="separator:a0548d3049128f34500d9212b7f946351"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a49b12ca60cc71722c665cb8c8032fcb0" id="r_a49b12ca60cc71722c665cb8c8032fcb0"><td class="memItemLeft" align="right" valign="top">subroutine, public&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacempi__utilities__mod.html#a49b12ca60cc71722c665cb8c8032fcb0">mpi_utilities_mod::get_global_max</a> (max)</td></tr>
<tr class="memdesc:a49b12ca60cc71722c665cb8c8032fcb0"><td class="mdescLeft">&#160;</td><td class="mdescRight">Collect global max values on each task.  <br /></td></tr>
<tr class="separator:a49b12ca60cc71722c665cb8c8032fcb0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8bd9d46f9aecd5100fb21f43fcadfc12" id="r_a8bd9d46f9aecd5100fb21f43fcadfc12"><td class="memItemLeft" align="right" valign="top">subroutine&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="mpif08__utilities__mod_8f90.html#a8bd9d46f9aecd5100fb21f43fcadfc12">exit_all</a> (exit_code)</td></tr>
<tr class="memdesc:a8bd9d46f9aecd5100fb21f43fcadfc12"><td class="mdescLeft">&#160;</td><td class="mdescRight">NOTE: non-module code, so this subroutine can be called from the utilities module, which this module uses (and cannot have circular refs)  <br /></td></tr>
<tr class="separator:a8bd9d46f9aecd5100fb21f43fcadfc12"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<h2 class="groupheader">Function/Subroutine Documentation</h2>
<a id="a8bd9d46f9aecd5100fb21f43fcadfc12" name="a8bd9d46f9aecd5100fb21f43fcadfc12"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8bd9d46f9aecd5100fb21f43fcadfc12">&#9670;&#160;</a></span>exit_all()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">subroutine exit_all </td>
          <td>(</td>
          <td class="paramtype">integer, intent(in)&#160;</td>
          <td class="paramname"><em>exit_code</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">private</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>NOTE: non-module code, so this subroutine can be called from the utilities module, which this module uses (and cannot have circular refs) </p>
<p>In case of error, call this instead of the fortran intrinsic exit(). It will signal the other MPI tasks that something bad happened and they should also exit. </p>

<p class="definition">Definition at line <a class="el" href="mpif08__utilities__mod_8f90_source.html#l02033">2033</a> of file <a class="el" href="mpif08__utilities__mod_8f90_source.html">mpif08_utilities_mod.f90</a>.</p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="mpif08__utilities__mod_8f90_a8bd9d46f9aecd5100fb21f43fcadfc12_cgraph.png" border="0" usemap="#ampif08__utilities__mod_8f90_a8bd9d46f9aecd5100fb21f43fcadfc12_cgraph" alt=""/></div>
<map name="ampif08__utilities__mod_8f90_a8bd9d46f9aecd5100fb21f43fcadfc12_cgraph" id="ampif08__utilities__mod_8f90_a8bd9d46f9aecd5100fb21f43fcadfc12_cgraph">
<area shape="rect" title="NOTE: non&#45;module code, so this subroutine can be called from the utilities module,..." alt="" coords="5,37,68,63"/>
<area shape="rect" href="namespacempi__utilities__mod.html#adc06ffeb863472ff64785cbd82cc36d7" title="return our communicator" alt="" coords="116,29,255,70"/>
<area shape="poly" title=" " alt="" coords="68,47,100,47,100,52,68,52"/>
<area shape="poly" title=" " alt="" coords="156,29,155,19,160,11,170,5,185,3,202,5,212,11,209,16,200,10,185,8,172,10,163,14,160,20,162,28"/>
</map>
</div>

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
